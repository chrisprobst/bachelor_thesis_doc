%!TEX root = bachelor.tex

\chapter{Evaluation}
\label{evaluation}

This chapter contains the evaluation of the framework. It starts with expectations and the used methodology in Section \ref{evaluation:expectations}. Then, different scenarios are described and evaluated in Section \ref{evaluation:1}, \ref{evaluation:2}, \ref{evaluation:3}. \ref{evaluation:456}, \ref{evaluation:78910} and \ref{evaluation:1112}.

\section{Expectations and Methodology}
\label{evaluation:expectations}

The efficiency of the different distribution models is already discussed theoretically in Chapter \ref{theory}. While the trade-offs are already known, the implementation of these models can cause unexpected side effects, which could have an impact on performance. The main goal of this evaluation is to verify, that the implementation of the Chunked-Swarm model is able to fulfill the $2\:*\:T_0$ limit in practice, as predicted in Section \ref{theory:model:chunkedswarm}. Therefore, the Benchmark module is used to simulate certain scenarios and thus to measure the influence of distribution algorithms and certain parameters like peer count, chunk count and meta data size. The Monitoring module, discussed in Section \ref{module:monitoring}, is used to record the current upload and download bandwidth and completion time of each peer during the benchmarks.

In order to get meaningful results, every scenario using the Chunked-Swarm model runs ten times, except those, which use the Sequential and Logarithmic model, because these models behave always predictable. In case of scenarios using the Chunked-Swarm model, the results are merged by calculating the mean and the confidence interval using a confidence level of 95\,\% of all runs. All results are plotted using GNUplot. There are twelve scenarios in total. The first scenario is considered the default, while the remaining scenarios are variations of it. To measure the influence of each parameter, the scenarios have always only one parameter, that is different to the default scenario. All scenarios ran in real-time on a virtual server with 14 \emph{Intel Xeon} 2.1 Ghz cores and 38 GB main memory.

\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s1:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_1_default/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s1:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_1_default/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s1:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_1_default/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s1:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_1_default/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s1:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_1_default/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 1 - Default}
		\label{fig:s1}
	\end{center}
\end{figure}


\section{Scenario 1: Default}
\label{evaluation:1}

Scenario 1, also called default scenario, simulates 64 peers using the Chunked-Swarm model, as explained in Section \ref{theory:model:chunkedswarm}, with super seeder extension, as discussed in Section \ref{module:algorithm:chunkedswarm}, where one peer has the complete data set at the beginning, which is called the super seeder and does only upload each chunk once. 

It should be noted, as explained in Section \ref{module:core:net}, that a peer contains both, a leecher and a seeder component. Programmatically, the leecher component is the downloading part and the seeder the uploading part of the peer. The peer with the complete data set at the beginning, which is called the super seeder, does not have a leecher component, because it is not interested in downloading data sets. The graphs use the same terminology, so graphs showing the seeder upload bandwidth actually show the peer upload bandwidth, while graphs showing the leecher download bandwidth refer to the peer download bandwidth.

Every peer has a simulated upload bandwidth of $16.384\:\frac{byte}{second}$. It is not necessary to limit the download bandwidth, because a peer always downloads chunks from a peer, whose upload bandwidth is limited anyway, so limiting the download bandwidth as well would increase the overhead without having any benefit. 

The small upload bandwidth has a major advantage for benchmarking, because a peer always has to allocate a buffer for each connection, whose size is relative to the bandwidth. If the upload bandwidth is too high, the overhead of the memory usage will influence the expressiveness of the benchmark.

The size of the data set is not specified directly, but is calculated from the upload bandwidth and $T_0$, which is set to 10 minutes (600 seconds). So a single transfer from the super seeder to one peer takes exactly $T_0$ seconds. Since the size of the data set is always relative to the upload bandwidth and $T_0$, the actual upload bandwidth does not matter anyway. 

The data set is also split into twice as many chunks as there are peers, but without the super seeder, which makes $63\:*\:2=126$ chunks. This scenario also simulates a meta data size of 1 byte. In reality, the meta data would require approximately 64 bytes, because it contains a \emph{SHA-1} hash, a bit set and some network protocol headers, but since the upload bandwidth is reduced for the benchmark, the meta data size should be reduced as well. In this specific case the $\frac{16.384\:\frac{byte}{second}}{1\:byte}$ ratio is equal to $\frac{1.048.576\:\frac{byte}{second}}{64\:byte}$. So if we assume, that a usual peer has an upload bandwidth of $1.048.576\:\frac{byte}{second}$, which is quite common these days, the benchmark values are realistic.

Scenario 13 and 14 use a meta data size of 0 and 10 bytes respectively, but the results are almost identical to those of this scenario, so the meta data size hardly influence the benchmark. The results can be found in Appendix \ref{appendix1}.

Figure \ref{fig:s1:completion} shows the mean completion graph for each peer, where the x-axis represents the time and the y-axis the completion of the data set. After $1.5\:*\:T_0$ seconds every peer has the complete data set available. As shown in Chapter \ref{theory}, the formula $T(n, c) = (1\:+\:\frac{n-1}{c})\:*\:T_0$ calculates the time needed for the Chunked-Swarm model. So $c\:=126$, $n\:=\:63$ and $T(63, 126) = (1\:+\:\frac{63-1}{126})\:*\:T_0 = 1,49\:*\:T_0$. Figure \ref{fig:s1:scompletion} shows the mean completion time of all 63 peers, sorted in descending order. Since this graph is almost a horizontal line, all peers complete the transfer roughly at the same time.

Figure \ref{fig:s1:ssupload} shows the mean super seeder upload bandwidth. For $T_0$ seconds, the super seeder uploads at full speed after which it stops uploading, because the super seeder extension forbids uploading the same chunk twice, so every chunk is uploaded once. Figure \ref{fig:s1:upload} and Figure \ref{fig:s1:download} present the mean upload and download bandwidth of all remaining peers. Since there are twice as many chunks as peers, each peer can start uploading chunks after $0.5\:*\:T_0$ seconds, as described in Section \ref{theory:model:chunkedswarm}. It is important to note, that the super seeder and the remaining peers upload in parallel after $0.5\:*\:T_0$ seconds.

The results show, that this model behaves exactly as predicted. It even undercuts the initial limit of $2\:*\:T_0$ seconds by far. The following scenarios \ref{evaluation:2} and \ref{evaluation:3} modify the default scenario by using the Sequential and Logarithmic model, which are described in Section \ref{theory:model:sequential} and \ref{theory:model:logarithmic} respectively.


\vfill
\pagebreak

\begin{figure}[ht]
	\begin{center}
		\subfigure[Completion Process\label{fig:s2:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_2_seq/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s2:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_2_seq/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}	 	

		\subfigure[Leecher Download Bandwidth\label{fig:s2:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_2_seq/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s2:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_2_seq/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}

		\caption{Scenario 2 - Sequential}
		\label{fig:s2}
	\end{center}
\end{figure}

\pagebreak
\section{Scenario 2: Sequential}
\label{evaluation:2}

Scenario 2 uses the Sequential model, as explained in Section \ref{theory:model:sequential}, so there is only one peer, which is uploading. This peer is also called super seeder, though it is not related to the super seeder extension of the Chunked-Swarm model. 

This super seeder uploads the chunks of the data set to all connected peers in parallel, which are not distributed by the peers themselves. This scenario represents a client\,/\,server system, where the upload bandwidth is shared by all connected peers. All other parameters are taken from the default scenario. As expected, all peers need approximately $63\:*\:T_0$ seconds to complete the transfer, as shown in Figure \ref{fig:s2:completion} and \ref{fig:s2:scompletion}. This makes sense, because every peer can only download with $\frac{16.384}{63}=267\:\frac{byte}{second}$, while the super seeder uploads at full speed, see Figure \ref{fig:s2:download} and \ref{fig:s2:ssupload}.

The repeating peaks in the upload and download bandwidth graphs are caused by the leaky-bucket algorithm used by the traffic shaping mechanism, see \ref{module:core:net:traffic}, and have no impact on performance. Initially, these peaks 
had a larger magnitude, so the graphs were reduced by plotting only the mean of ten values at a time.

\vfill


\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s3:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_3_log/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s3:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_3_log/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s3:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_3_log/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s3:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_3_log/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s3:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_3_log/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 3 - Logarithmic}
		\label{fig:s3}
	\end{center}
\end{figure}

\pagebreak
\section{Scenario 3: Logarithmic}
\label{evaluation:3}

Scenario 3 uses the Logarithmic model. As explained in Section \ref{theory:model:logarithmic}, this model uses a Peer-to-Peer network, like the Chunked-Swarm model, but only complete data sets are transferred and a single peer only uploads to one other peer in parallel. There is also only one peer, which has the complete data set at first and is called the super seeder. This way, the number of uploading peers doubles every $T_0$ seconds, so the number of uploading and completed peers grows exponentially, see Figure \ref{fig:s3:completion} and \ref{fig:s3:scompletion}. Thus, there are never more than 50\,\% of all peers uploading in parallel, which is shown in Figure \ref{fig:s3:upload} and \ref{fig:s3:download}. Figure \ref{fig:s3:ssupload} presents the super seeder upload bandwidth including some rough peaks, which are caused by the announce delay. When a peer completes a download, it takes a few seconds to notify the remaining peers in the network. During this time the super seeder is not uploading anything, so these peaks are actually expected. Like scenario 2, the minor peaks are part of the traffic shaping mechanism.

\vfill


\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s4:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_8_peer_count_32/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s4:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_8_peer_count_32/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s4:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_8_peer_count_32/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s4:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_8_peer_count_32/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s4:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_8_peer_count_32/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 4 - 32 Peers}
		\label{fig:s4}
	\end{center}
\end{figure}
\vfill

\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s5:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_4_peer_count_128/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s5:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_4_peer_count_128/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s5:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_4_peer_count_128/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s5:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_4_peer_count_128/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s5:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_4_peer_count_128/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 5 - 128 Peers}
		\label{fig:s5}
	\end{center}
\end{figure}
\vfill

\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s6:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_11_peer_count_192_v2/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s6:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_11_peer_count_192_v2/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s6:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_11_peer_count_192_v2/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s6:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_11_peer_count_192_v2/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s6:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_11_peer_count_192_v2/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 6 - 192 Peers}
		\label{fig:s6}
	\end{center}
\end{figure}
\vfill

\pagebreak
\section{Scenario 4, 5 and 6: Peer Count}
\label{evaluation:456}

Compared to the default scenario, scenario 4, 5 and 6 have a peer count of 32, 128 and 192 respectively. These scenarios are used to observe the large-scale behavior, whose results are shown in Figure \ref{fig:s4}, \ref{fig:s5} and \ref{fig:s6}. The results from scenario 4 and the default scenario are almost identical. In contrast, scenario 5 and 6 show a minor decrease in performance. While the default scenario and scenario 4 both take about $1.5\:*\:T_0$ seconds, scenario 5 and 6 take $1.6\:*\:T_0$ and $1.7\:*\:T_0$ seconds respectively. While all of these scenarios do not exceed $2\:*\:T_0$ seconds, the performance obviously drops when using more peers.

If $n$ is the number of peers, the whole Peer-to-Peer network consists of $(n-1)\:*\:n$ connections, which is one reason for the performance drop. If the peer count is doubled, the chunk count is doubled as well, so the number of announcements actually quadruples. Theoretically, the Chunked-Swarm model works for any number of clients, but in practice the overhead introduced by the growing number of chunks and announce messages is just too high at some point. In reality, every announce message would also increase the latency caused by the \emph{round trip time}. 

Also, the scenarios ran in real-time, so if the CPU is utilized by 95\,\% or more, the results will lose significance. A solution for this problem is discussed in Chapter \ref{conclusion}. Another reason might be unfair chunk distribution, because if a peer downloads its chunk too fast, it might download a second chunk, which should have been distributed by another peer and thus distributes two chunks instead of one, as shown in Figure \ref{fig:s5:upload} and \ref{fig:s6:upload}. Some peers start distributing their own chunks before $0.5\:*\:T_0$ seconds, while others seem to upload chunks even after $1.5\:*\:T_0$ seconds. A high CPU utilization can also pander this effect.

It should be noted, that the limit is not exactly 192 peers. When simulating 192 peers, the server actually runs $192\:*\:191=36.672$ connections at once, which has a great impact on CPU and main memory usage. Programmatically, there are also major reasons for the huge overhead. Since the implementation is written in Java, there is no way to allocate class instances on the stack. This is discussed in Chapter \ref{conclusion} in more detail. Also, in reality, the overhead is distributed evenly among all peers. So a single peer should be able to handle way more than 192 peers. Unfortunately, there are no results taken from real peers yet, so the last statement is just an assumption, which has yet to be proven.

\vfill



\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s7:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_7_chunk_count_fac_1/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s7:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_7_chunk_count_fac_1/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s7:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_7_chunk_count_fac_1/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s7:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_7_chunk_count_fac_1/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s7:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_7_chunk_count_fac_1/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 7 - Chunk Count Factor 1}
		\label{fig:s7}
	\end{center}
\end{figure}
\vfill

\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s8:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_15_chunk_count_fac_4/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s8:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_15_chunk_count_fac_4/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s8:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_15_chunk_count_fac_4/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s8:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_15_chunk_count_fac_4/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s8:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_15_chunk_count_fac_4/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 8 - Chunk Count Factor 4}
		\label{fig:s8}
	\end{center}
\end{figure}
\vfill

\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s9:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_16_chunk_count_fac_8/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s9:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_16_chunk_count_fac_8/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s9:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_16_chunk_count_fac_8/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s9:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_16_chunk_count_fac_8/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s9:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_16_chunk_count_fac_8/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 9 - Chunk Count Factor 8}
		\label{fig:s9}
	\end{center}
\end{figure}
\vfill


\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s10:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_17_chunk_count_fac_16/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s10:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_17_chunk_count_fac_16/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s10:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_17_chunk_count_fac_16/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s10:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_17_chunk_count_fac_16/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s10:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_17_chunk_count_fac_16/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 10 - Chunk Count Factor 16}
		\label{fig:s10}
	\end{center}
\end{figure}
\vfill

\pagebreak
\section{Scenario 7, 8, 9 and 10: Chunk Count Factor}
\label{evaluation:78910}

In scenario 7, 8, 9 and 10 the chunk count factor is adjusted to 1, 4, 8 and 16 respectively. The chunk count factor is multiplied with the number of peers, but without the super seeder, to calculate the actual chunk count. The default scenario uses a chunk count factor of 2 and a peer count of 63, without the super seeder. Since these four scenarios only change the chunk count factor, the used chunk counts are $1\:*\:63$, $4\:*\:63=252$, $8\:*\:63=504$ and $16\:*\:63=1008$ for scenario 7, 8, 9 and 10 respectively.   

As expected, scenario 7 completes in $2\:*\:T_0$ seconds, which is shown by Figure \ref{fig:s7}. In the first $T_0$ seconds the super seeder uploads a distinct chunk to each peer, whereby the complete data set is available in the network. Since the super seeder is not allowed to upload the same chunk twice, peers have to distribute all chunks evenly among themselves, which also takes roughly $T_0$ seconds. These results correspond with the formula presented in \ref{theory:model:chunkedswarm}, because $c\:=63$, $n\:=\:63$ and $T(63, 63) = (1\:+\:\frac{63-1}{63})\:*\:T_0 = 1,98\:*\:T_0$.

Scenario 8, whose results are shown in Figure \ref{fig:s8}, completed in $1.3\:*\:T_0$ seconds, which is almost the expected duration, since $c\:=252$, $n\:=\:63$ and $T(63, 252) = (1\:+\:\frac{63-1}{252})\:*\:T_0 = 1,25\:*\:T_0$. But it is important to note, with chunk count factor of 4, the completion graph already starts to leave the expected path, which should have been a step function with four steps, starting at $0.25\:*\:T_0$ seconds. As with scenario 5 and 6, the reasons for this observation are an exponentially growing amount of announcements and a higher CPU usage, which affects the measurements done in real-time and provokes unfair chunk distribution.

Scenario 9 and 10 overstrain the CPU even more, as shown in Figure \ref{fig:s9} and \ref{fig:s10}. The expected durations are $(1\:+\:\frac{63-1}{504})\:*\:T_0 = 1,12\:*\:T_0$ and $(1\:+\:\frac{63-1}{1008})\:*\:T_0 = 1,06\:*\:T_0$ respectively. However, according to Figure \ref{fig:s9:scompletion} and \ref{fig:s10:scompletion}, scenario 9 and 10 complete on average at about $1.18\:*\:T_0$ and $1.22\:*\:T_0$ seconds respectively. While scenario 9 is barely acceptable, scenario 10 has a new influence, that should be noted. As explained before, the results are calculated from ten runs. At this point, all ten runs of all scenarios completed mostly identical, which is represented by the low confidence interval. This changes dramatically in scenario 10. Some runs completed at $1.5\:*\:T_0$, while the majority completed at $1.08\:*\:T_0$. This means, that some runs behaved much worse than the others, which is represented by high confidence intervals in Figure \ref{fig:s10:scompletion}, \ref{fig:s10:upload} and \ref{fig:s10:download}. These results let assume, that unfair chunk distribution is now the main reason for this imbalance.

In theory, if the chunk count is doubled, the completion time after $T_0$ seconds will cut in half. Scenario 1, 7, 8 and 9 confirm this assumption. But it seems, that unfair chunk distribution can bring the system out of step. Since
the possibility for unfair chunk distribution grows equally with CPU usage and chunk count, certain control mechanisms should be implemented in a future release, to make sure, each peer distributes only the chunk it is responsible for. 

\vfill


\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s11:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_9_parts_10/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s11:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_9_parts_10/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s11:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_9_parts_10/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s11:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_9_parts_10/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s11:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_9_parts_10/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 11 - Streaming 10 Data Sets}
		\label{fig:s11}
	\end{center}
\end{figure}
\vfill


\pagebreak
\begin{figure}[!ht]
	\begin{center}	
		\subfigure[Completion Process\label{fig:s12:completion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_6_parts_20/plots/GeneratedMeanChunkCompletion.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Sorted Completion\label{fig:s12:scompletion}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_6_parts_20/plots/GeneratedMeanSortedChunkCompletion.csv}
	 	}		

	 	\subfigure[Super Seeder Upload Bandwidth\label{fig:s12:ssupload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_6_parts_20/plots/GeneratedMeanCurrentSuperSeederUploadBandwidth.csv}
	 	}~ % No whitespace here!
	 	\subfigure[Seeder Upload Bandwidth\label{fig:s12:upload}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_6_parts_20/plots/GeneratedMeanCurrentUploadBandwidth.csv}
	 	}

	 	\subfigure[Leecher Download Bandwidth\label{fig:s12:download}]{
	 		\includegraphics[width=0.5\textwidth]{plots/scenario_6_parts_20/plots/GeneratedMeanCurrentDownloadBandwidth.csv}
	 	}
		\caption{Scenario 12 - Streaming 20 Data Sets}
		\label{fig:s12}
	\end{center}
\end{figure}
\vfill

\pagebreak
\section{Scenario 11 and 12: Streaming Multiple Data Sets}
\label{evaluation:1112}

So far, all other scenarios used and transferred only one data set. The last scenarios 11 and 12 implement the streaming of 10 and 20 data sets respectively, see Section \ref{theory:model:chunkedswarm} and \ref{module:streaming}. The data sets are much smaller now and thus complete in less time.

In case of scenario 11, each data set takes $\frac{1}{10}\:*\:T_0$ seconds to get from one peer to another. So transferring all data sets take $T_0$ seconds. Therefore, the peers download ten data sets from the super seeder and distribute them one after the other. This is useful to implement sequential streaming, where a huge data source is split into multiple data sets, which are downloaded and distributed sequentially so that each peer do not have random chunks but a growing amount of coherent chunks instead. Video streaming is only one domain, which relies on this behavior.

Figure \ref{fig:s11} shows the results from scenario 11. An interesting observation is, that all peers finish within $1.2\:*\:T_0$ seconds, but each data set has a chunk count factor of 2. The reason is, that the download and the distribution phases of each data set interleave. So the super seeder starts uploading the next data set while the peers are still distributing the last data set, which is shown in Figure \ref{fig:s11:ssupload}, \ref{fig:s11:upload} and \ref{fig:s11:download}. The high confidence intervals are caused by the large amount of chunks. When simulating ten runs, it is very unlikely, that all results are equal. But the confidence intervals show, that it is still very likely, that a further run will be very similar. The completion graphs \ref{fig:s11:completion} and \ref{fig:s11:scompletion} also show the good distribution behavior. All peers download and distribute the data sets equally. Figure \ref{fig:s12} contains the results from scenario 12, which has 20 data sets instead of 10. The results are very similar to those from scenario 11, so the observations can be transferred.

While the results of both scenarios look promising, it should be noted, that both scenarios ran 20 times instead of 10. Then, only the best 10 runs were chosen and merged. The reason is, that some runs took about $4\:*\:T_0$ to $6\:*\:T_0$ seconds to finish, while others took roughly $1.2\:*\:T_0$ seconds. In case of the slow scenarios, the results show, that most of the peers were not uploading any more after $1.5\:*\:T_0$ seconds, which is the reason why these runs took so long. So the graphs of both scenarios show, that it is possible to get the expected performance, but the system can get out of step. Again, the main reason for this effect is unfair chunk distribution. In the next Chapter \ref{conclusion} a push-based approach is proposed, which should solve this particular problem in the future.
