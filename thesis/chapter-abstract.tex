%!TEX root = bachelor.tex

\pdfbookmark[0]{Abstract}{abstract}
\begin{center} 
\huge Abstract
\end{center}


This thesis concentrates on implementing and evaluating different distribution algorithms for generic data transfers in large-scale Peer-to-Peer networks, especially for incremental transfers, which yields the possibillity for video streaming. The main focus is on $1:n$ scenarios, where only one peer has a complete data set and $n$ peers try to distribute this data set among themselves as fast as possible.

The main problem is the choice of the best distribution algorithm, which should be able to use most of the available network bandwidth of all participating peers. In a traditional client\,/\,server system, the server uploads the data set to all clients sequentially, which means that only the server upload bandwidth is used but none of the client upload bandwidths.

Peer-to-Peer networks in combination with efficient distribution algorithms can help to solve this particular problem. These algorithms are always based on the same technique, where all participating peers download specific chunks of a data set from the peer, which has the complete data set and upload these chunks to other peers as well. This way, the peers help to distribute the data set using their own upload bandwidth. To distribute the data sets, a Peer-to-Peer network can either use a pull-based approach, where each peer explicitely requests chunks from other peers, or a push-based approach, where the peers transfer chunks without being asked to do so. Which approach to use also depends on the topology of the Peer-to-Peer network. Other parameters like chunk count and chunk size should also be chosen carefully.

In the course of this thesis, three pull-based distribution models are developed and evaluated. The main goal is to find a distribution model, that can distribute a data set evenly among all peers within $2\:*\:T_0$ seconds, where $T_0$ is the time to transfer the data set from one peer to another. Of these three models, only the Chunked-Swarm model is able to fulfill this limit. Therefore, the majority of the evaluation is dedicated to this model.

The evaluation of the Chunked-Swarm model shows, that this model works fine in general. Using appropriate parameters, this model can even undercut the $2\:*\:T_0$ limit significantly. All benchmarks run in real-time, so a high CPU usage can affect the significance of the results, which is the case for scenarios with a large number of peers, since the mesh topology adds exponentially growing overhead. Further evaluation should be based on a simulation time, to remove the influence of a high CPU usage.